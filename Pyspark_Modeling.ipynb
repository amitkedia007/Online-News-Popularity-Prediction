{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilFpGsT0RuZ8"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcpOdVluSAHv"
      },
      "outputs": [],
      "source": [
        "import pyspark "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD_EAfiaSdTR"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRJlPJfbSt8p"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "news_popularity = pd.read_csv(io.BytesIO(uploaded['OnlineNewsPredition_Reduced.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzB5G2qWT1rM"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "import time\n",
        "import timeit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd0ziz0PUIRS"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('DDA_Assignment').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TA-pgLZUgyf"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOt1ytm3Ujxl"
      },
      "outputs": [],
      "source": [
        "df_pyspark = spark.read.csv('OnlineNewsPredition_Reduced.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILbVWsI-cN_u"
      },
      "outputs": [],
      "source": [
        "df_pyspark = spark.read.option('header', 'true').csv('OnlineNewsPredition_Reduced.csv', inferSchema= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V_CU8ukcUb9"
      },
      "outputs": [],
      "source": [
        "type(df_pyspark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amHXiigScuVv"
      },
      "outputs": [],
      "source": [
        "df_pyspark.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qurbBqtac2X3"
      },
      "outputs": [],
      "source": [
        "feature_columns = [column for column in df_pyspark.columns if column != 'shares']\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting train test data"
      ],
      "metadata": {
        "id": "p-dlMqh9YuMZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5pmpaf_wHU7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
        "df_pandas = df_pyspark.toPandas()\n",
        "\n",
        "# Separate the features (X) and the target variable (y)\n",
        "target_column = 'shares'\n",
        "X = df_pandas.drop(target_column, axis=1)\n",
        "y = df_pandas[target_column]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "LlcCJk1OYzJS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIqylW8G0JHM"
      },
      "outputs": [],
      "source": [
        "# WE try to use the cross validation without pyspark but it requires huge computational resourses. So we are not able to run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9FVfTi55aQj"
      },
      "outputs": [],
      "source": [
        "feature_columns = df_pyspark.columns[:-1]  # assuming 'shares' is the last column in your DataFrame\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rRsALJ48XNn"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(labelCol=\"shares\", featuresCol=\"features\", seed=42, numTrees=500) # Using 500 trees to train our model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3aQG6n28ZDG"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = df_pyspark.randomSplit([0.8, 0.2], seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlXi0-uy8aGQ"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "model = pipeline.fit(train_data)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Execution time: {elapsed_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlKQA5sp8bjN"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-UCvtRa8eDJ"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "evaluator = RegressionEvaluator(labelCol=\"shares\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Execution time: {elapsed_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnDB-Xt-8fge"
      },
      "outputs": [],
      "source": [
        "evaluator_mae = RegressionEvaluator(labelCol=\"shares\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator_mae.evaluate(predictions)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m2vG4BWDZGN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}