{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr68a1LW_oSm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns08oYb9_2UW"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dtB0PMCZo4KB"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wC-dwXP_-_C"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "news_popularity = pd.read_csv(io.BytesIO(uploaded['OnlineNewsPopularity.csv']))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7NYKn3VCIZ8r"
      },
      "source": [
        "First need to check the first 10 rows of the dataset to understand the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aTimlWpIM0L"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "news_popularity.head(10)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7GJpcgP3Iv81"
      },
      "source": [
        "Now lets see the dimension of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b3zxil9ISgQ"
      },
      "outputs": [],
      "source": [
        "news_popularity.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8qHWcNZmJqpS"
      },
      "source": [
        "The dataset has 39644 rows and 60 variables and one target variable."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BluqwNo0MjX0"
      },
      "source": [
        "Now we need to check the quality of the dataset. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0DRGt7SFbNys"
      },
      "source": [
        "First we need to know the variabels names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlaDk-7jhFyA"
      },
      "outputs": [],
      "source": [
        "news_popularity = pd.DataFrame(news_popularity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXhZYlcWI-s6"
      },
      "outputs": [],
      "source": [
        "column_names = news_popularity.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ77OaSAhxq2"
      },
      "outputs": [],
      "source": [
        "column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru5H9RwrqVjH"
      },
      "outputs": [],
      "source": [
        "# Removed the url column from the dataframe.\n",
        "df = news_popularity.drop(\"url\", axis = 1)\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zK_zq__bUOVD"
      },
      "source": [
        "# Understand the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxK6iB28pfn4"
      },
      "outputs": [],
      "source": [
        "data_types = pd.DataFrame(df.dtypes, columns=['data_type'])\n",
        "data_types"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4UHliS7-jDvx"
      },
      "source": [
        "From the above output we can say that all the variables are numerical and according to the need we will convert the binary variables into category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW5XVUJiifU-"
      },
      "outputs": [],
      "source": [
        "# Satistics of data\n",
        "df.describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iK-ujvaJisHV"
      },
      "source": [
        "By checking the summary statistics of the dataset we got to know that their might be outliers, implausible data and some of datatype is wrongly interpreted by Python\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zMAVeQ1N11c7"
      },
      "source": [
        "# Check Missing Values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3p8FehtWe_A"
      },
      "outputs": [],
      "source": [
        "features_with_na = [features for features in df.columns]\n",
        "\n",
        "for features in features_with_na:\n",
        "  print(features, np.round(df[features].isnull().mean(), 4), ' % Missing Values')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MrjMBPtu5lym"
      },
      "source": [
        "# Separating Numerical & Categorical Variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZsFX3fgWfBo"
      },
      "outputs": [],
      "source": [
        "my_features = [features for features in df.columns]\n",
        "\n",
        "# Separating continuous and discrete variables\n",
        "continuous_features = []\n",
        "discrete_features = []\n",
        "\n",
        "for feature in my_features:\n",
        "    unique_values = df[feature].nunique()\n",
        "    if unique_values > 3:  # Adjust the threshold depending on your dataset\n",
        "        continuous_features.append(feature)\n",
        "    else:\n",
        "        discrete_features.append(feature)\n",
        "\n",
        "print(\"Continuous Variables Count: {}, Continuous features: {}\".format(len(continuous_features), continuous_features))\n",
        "print(\"Discrete Variables Count: {}, Discrete features: {}\".format(len(discrete_features), discrete_features))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2NBJ7ju7JJ-"
      },
      "source": [
        "We have 45 continuous variables and 15 categorical variables. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY_RhcTHfvaB"
      },
      "outputs": [],
      "source": [
        "# Change the datatype of 14 Categorical variables\n",
        "for feature in [' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend']:\n",
        "    df[feature] = df[feature].astype('category')\n",
        "\n",
        "#check again\n",
        "data_types = pd.DataFrame(df.dtypes, columns=['data_type'])\n",
        "data_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtuMr54zAYTR"
      },
      "outputs": [],
      "source": [
        "df_continuous = df[continuous_features]\n",
        "df_descrete = df[discrete_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkSY3Oqk81hS"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot boxplot for each continuous feature\n",
        "for feature in continuous_features:\n",
        "    plt.figure()  # This will create a new figure for each feature\n",
        "    sns.boxplot(data=df_continuous, x=feature)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EBQFa0dJGH-_"
      },
      "source": [
        "As we can see there are outliers in most of the features. But first we need to check the implausible values in our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPwZpb61MGMH"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "liJj7JWCIOpB"
      },
      "source": [
        "# Check Implausible Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7o3zBY5IT_f"
      },
      "outputs": [],
      "source": [
        "# Define the plausible ranges for each feature\n",
        "feature_ranges = {\n",
        "    ' n_tokens_title': (1, None),\n",
        "    ' n_tokens_content': (0, None),\n",
        "    \" n_unique_tokens\": (0, 1),\n",
        "    \" n_non_stop_words\": (0, 1),\n",
        "    \" n_non_stop_unique_tokens\": (0, 1),\n",
        "    \" num_hrefs\": (0, None),\n",
        "    \" num_self_hrefs\": (0, None),\n",
        "    \" num_imgs\": (0, None),\n",
        "    \" num_videos\": (0, None),\n",
        "    \" average_token_length\": (1, None),\n",
        "    \" num_keywords\": (0, None),\n",
        "    \" kw_min_min\": (0, None),\n",
        "    \" kw_max_min\": (0, None),\n",
        "    \" kw_avg_min\": (0, None),\n",
        "    \" kw_min_max\": (0, None),\n",
        "    \" kw_max_max\": (0, None),\n",
        "    \" kw_avg_max\": (0, None),\n",
        "    \" kw_min_avg\": (0, None),\n",
        "    \" kw_max_avg\": (0, None),\n",
        "    \" kw_avg_avg\": (0, None),\n",
        "    \" self_reference_min_shares\": (0, None),\n",
        "    \" self_reference_max_shares\": (0, None),\n",
        "    \" self_reference_avg_sharess\": (0, None),\n",
        "    \" LDA_00\": (0, 1),\n",
        "    \" LDA_01\": (0, 1),\n",
        "    \" LDA_02\": (0, 1),\n",
        "    \" LDA_03\": (0, 1),\n",
        "    \" LDA_04\": (0, 1),\n",
        "    \" global_subjectivity\": (0, 1),\n",
        "    \" global_sentiment_polarity\": (-1, 1),\n",
        "    \" global_rate_positive_words\": (0, 1),\n",
        "    \" global_rate_negative_words\": (0, 1),\n",
        "    \" rate_positive_words\": (0, 1),\n",
        "    \" rate_negative_words\": (0, 1),\n",
        "    \" avg_positive_polarity\": (0, 1),\n",
        "    \" min_positive_polarity\": (0, 1),\n",
        "    \" max_positive_polarity\": (0, 1),\n",
        "    \" avg_negative_polarity\": (-1, 0),\n",
        "    \" min_negative_polarity\": (-1, 0),\n",
        "    \" max_negative_polarity\": (-1, 0),\n",
        "    \" title_subjectivity\": (0, 1),\n",
        "    \" title_sentiment_polarity\": (-1, 1),\n",
        "    \" abs_title_subjectivity\": (0, 1),\n",
        "    \" abs_title_sentiment_polarity\": (0, 1),\n",
        "    \" shares\": (0, None),\n",
        "}\n",
        "\n",
        "# Check the range for each feature and print which features have out of the given range values\n",
        "for feature, (min_value, max_value) in feature_ranges.items():\n",
        "    outside_range = False\n",
        "\n",
        "    if min_value is not None:\n",
        "        outside_range = outside_range or (df[feature] < min_value).any()\n",
        "\n",
        "    if max_value is not None:\n",
        "        outside_range = outside_range or (df[feature] > max_value).any()\n",
        "\n",
        "    if outside_range:\n",
        "        print(f\"{feature} has values outside the given range.\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WedMPsdIbvlZ"
      },
      "source": [
        "For rate features like n_unique_tokens, n_non_stop_words, and n_non_stop_unique_tokens, we impute out-of-range values with the minimum or maximum plausible value (0 or 1) as they represent proportions.\n",
        "For average_token_length, since the minimum length of a word is 1, we impute values below 1 with 1.\n",
        "For keyword-related features like kw_min_min, kw_avg_min, and kw_min_avg, we impute values below 0 with 0, because the number of shares cannot be negative.\n",
        "This imputation strategy ensures that the values fall within the plausible ranges for each feature while preserving the original data as much as possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQXuOjrmNBca"
      },
      "outputs": [],
      "source": [
        "# Impute out-of-range values with appropriate values\n",
        "# We'll use the .loc[] method to modify the values in-place\n",
        "\n",
        "# n_unique_tokens: Rate of unique words in the content, should be between 0 and 1\n",
        "# Impute values < 0 with 0, and values > 1 with 1\n",
        "df.loc[df[' n_unique_tokens'] < 0, ' n_unique_tokens'] = 0\n",
        "df.loc[df[' n_unique_tokens'] > 1, ' n_unique_tokens'] = 1\n",
        "\n",
        "# n_non_stop_words: Rate of non-stop words in the content, should be between 0 and 1\n",
        "# Impute values < 0 with 0, and values > 1 with 1\n",
        "df.loc[df[' n_non_stop_words'] < 0, ' n_non_stop_words'] = 0\n",
        "df.loc[df[' n_non_stop_words'] > 1, ' n_non_stop_words'] = 1\n",
        "\n",
        "# n_non_stop_unique_tokens: Rate of unique non-stop words in the content, should be between 0 and 1\n",
        "# Impute values < 0 with 0, and values > 1 with 1\n",
        "df.loc[df[' n_non_stop_unique_tokens'] < 0, ' n_non_stop_unique_tokens'] = 0\n",
        "df.loc[df[' n_non_stop_unique_tokens'] > 1, ' n_non_stop_unique_tokens'] = 1\n",
        "\n",
        "# average_token_length: Average length of the words in the content, should be >= 1\n",
        "# Impute values < 1 with 1 (shortest possible word length)\n",
        "df.loc[df[' average_token_length'] < 1, ' average_token_length'] = 1\n",
        "\n",
        "# kw_min_min: Worst keyword (min. shares), should be >= 0\n",
        "# Impute values < 0 with 0, as shares cannot be negative\n",
        "df.loc[df[' kw_min_min'] < 0, ' kw_min_min'] = 0\n",
        "\n",
        "# kw_avg_min: Worst keyword (avg. shares), should be >= 0\n",
        "# Impute values < 0 with 0, as shares cannot be negative\n",
        "df.loc[df[' kw_avg_min'] < 0, ' kw_avg_min'] = 0\n",
        "\n",
        "# kw_min_avg: Avg. keyword (min. shares), should be >= 0\n",
        "# Impute values < 0 with 0, as shares cannot be negative\n",
        "df.loc[df[' kw_min_avg'] < 0, ' kw_min_avg'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkreJN39Ovp6"
      },
      "outputs": [],
      "source": [
        "# Again checking the range to see if there is any feature with outside the given range \n",
        "for feature, (min_value, max_value) in feature_ranges.items():\n",
        "    outside_range = False\n",
        "\n",
        "    if min_value is not None:\n",
        "        outside_range = outside_range or (df[feature] < min_value).any()\n",
        "\n",
        "    if max_value is not None:\n",
        "        outside_range = outside_range or (df[feature] > max_value).any()\n",
        "\n",
        "    if outside_range:\n",
        "        print(f\"{feature} has values outside the given range.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cgX3VeNfPH05"
      },
      "source": [
        "No output, that means none of the features have out of range values. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ou_TBXj0PU7x"
      },
      "source": [
        "# Assessing the impact of outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chv7gF1KGPjC"
      },
      "outputs": [],
      "source": [
        "for column in continuous_features:    \n",
        "    q1 = df[column].quantile(0.25)    # First Quartile\n",
        "    q3 = df[column].quantile(0.75)    # Third Quartile\n",
        "    IQR = q3 - q1                            # Inter Quartile Range\n",
        "\n",
        "    llimit = q1 - 1.5*IQR                       # Lower Limit\n",
        "    ulimit = q3 + 1.5*IQR                        # Upper Limit\n",
        "\n",
        "    outliers = df[(df[column] < llimit) | (df[column] > ulimit)]\n",
        "    print('Number of outliers in \"' + column + '\" : ' + str(len(outliers)))\n",
        "    print(llimit)\n",
        "    print(ulimit)\n",
        "    print(IQR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wK40-c23Inkc"
      },
      "source": [
        "There are some features which have high number of outliers but could be plausible values. Therefore, to reduce the impact of it we will scale all the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ1aiuwVeDjc"
      },
      "outputs": [],
      "source": [
        "df[' shares'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKXnWoltc8zd"
      },
      "outputs": [],
      "source": [
        "threshold = df[' shares'].quantile(0.95)  # Calculate the threshold for the top 5% most viral news\n",
        "viral_news = df[df[' shares'] > threshold]  # Filter the DataFrame to get only the viral news\n",
        "\n",
        "print(\"Top 5% viral news threshold:\", threshold)\n",
        "print(\"Number of viral news articles:\", len(viral_news))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltAO1WCwdJxu"
      },
      "outputs": [],
      "source": [
        "threshold2 = df[' shares'].quantile(0.99)  # Calculate the threshold for the top 1% most viral news\n",
        "viral_news2 = df[df[' shares'] > threshold2]  # Filter the DataFrame to get only the viral news\n",
        "\n",
        "print(\"Top 1% viral news threshold:\", threshold2)\n",
        "print(\"Number of viral news articles:\", len(viral_news2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0jlUmVhdnTT"
      },
      "outputs": [],
      "source": [
        "# Create a scatterplot for 'n_tokens_content' vs 'shares'\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "palette = {True: \"mediumorchid\", False: \"cornflowerblue\"}\n",
        "\n",
        "# Create the scatterplot\n",
        "sns.scatterplot(data=df, x=' n_tokens_content', y=' shares', hue=(df[' shares'] > threshold), palette=palette)\n",
        "\n",
        "# Add a horizontal line representing the top 5% viral news threshold\n",
        "plt.axhline(y=threshold, color='crimson', linestyle='--', label='Top 5% viral news threshold')\n",
        "\n",
        "# Customize the plot appearance\n",
        "plt.legend()\n",
        "plt.title('Scatterplot of n_tokens_content vs shares', fontsize=16)\n",
        "plt.xlabel('n_tokens_content', fontsize=14)\n",
        "plt.ylabel('shares', fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oihXKOo5e3zq"
      },
      "source": [
        "So we can notice here that the outliers can be a possible as people less tend to read the news which have high number of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBr3DuV_gB1g"
      },
      "outputs": [],
      "source": [
        "#n_non_stop_words\n",
        "\n",
        "# Create a scatterplot for 'n_non_stop_words' vs 'shares'\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "palette = {True: \"mediumorchid\", False: \"cornflowerblue\"}\n",
        "\n",
        "# Create the scatterplot\n",
        "sns.scatterplot(data=df, x=' n_non_stop_words', y=' shares', hue=(df[' shares'] > threshold), palette=palette)\n",
        "\n",
        "# Add a horizontal line representing the top 5% viral news threshold\n",
        "plt.axhline(y=threshold, color='crimson', linestyle='--', label='Top 5% viral news threshold')\n",
        "\n",
        "# Customize the plot appearance\n",
        "plt.legend()\n",
        "plt.title('Scatterplot of n_non_stop_words vs shares', fontsize=16)\n",
        "plt.xlabel('n_non_stop_words', fontsize=14)\n",
        "plt.ylabel('shares', fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JMYXZgEgkBJ"
      },
      "outputs": [],
      "source": [
        "# Create a scatterplot for 'kw_max_max' vs 'shares'\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "palette = {True: \"mediumorchid\", False: \"cornflowerblue\"}\n",
        "\n",
        "# Create the scatterplot\n",
        "sns.scatterplot(data=df, x=' kw_max_max', y=' shares', hue=(df[' shares'] > threshold), palette=palette)\n",
        "\n",
        "# Add a horizontal line representing the top 5% viral news threshold\n",
        "plt.axhline(y=threshold, color='crimson', linestyle='--', label='Top 5% viral news threshold')\n",
        "\n",
        "# Customize the plot appearance\n",
        "plt.legend()\n",
        "plt.title('Scatterplot of kw_max_max vs shares', fontsize=16)\n",
        "plt.xlabel('kw_max_max', fontsize=14)\n",
        "plt.ylabel('shares', fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfLQStseriGy"
      },
      "outputs": [],
      "source": [
        "# Create a scatterplot for 'n_unique_tokens' vs 'shares'\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "palette = {True: \"mediumorchid\", False: \"cornflowerblue\"}\n",
        "\n",
        "# Create the scatterplot\n",
        "sns.scatterplot(data=df, x=' n_unique_tokens', y=' shares', hue=(df[' shares'] > threshold), palette=palette)\n",
        "\n",
        "# Add a horizontal line representing the top 5% viral news threshold\n",
        "plt.axhline(y=threshold, color='crimson', linestyle='--', label='Top 5% viral news threshold')\n",
        "\n",
        "# Customize the plot appearance\n",
        "plt.legend()\n",
        "plt.title('Scatterplot of n_unique_tokens vs shares', fontsize=16)\n",
        "plt.xlabel('n_unique_tokens', fontsize=14)\n",
        "plt.ylabel('shares', fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myuDuvuetHzj"
      },
      "outputs": [],
      "source": [
        "# 'kw_avg_max' \n",
        "# Create a scatterplot for 'kw_avg_max' vs 'shares'\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "palette = {True: \"mediumorchid\", False: \"cornflowerblue\"}\n",
        "\n",
        "# Create the scatterplot\n",
        "sns.scatterplot(data=df, x=' kw_avg_max', y=' shares', hue=(df[' shares'] > threshold), palette=palette)\n",
        "\n",
        "# Add a horizontal line representing the top 5% viral news threshold\n",
        "plt.axhline(y=threshold, color='crimson', linestyle='--', label='Top 5% viral news threshold')\n",
        "\n",
        "# Customize the plot appearance\n",
        "plt.legend()\n",
        "plt.title('Scatterplot of kw_avg_max vs shares', fontsize=16)\n",
        "plt.xlabel('kw_avg_max', fontsize=14)\n",
        "plt.ylabel('shares', fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W80q4lONJIa5"
      },
      "source": [
        "# Scaling of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83iHX18tIxyp"
      },
      "outputs": [],
      "source": [
        "# We have sperated continuous and descrete features. \n",
        "continuous_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxVfsq0jJmIr"
      },
      "outputs": [],
      "source": [
        "discrete_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI3mGpwEJ3IT"
      },
      "outputs": [],
      "source": [
        "# As we have only list of continuous_features and discrete_features. Therefore it is necessary to convert into dataframe to perform future analysis\n",
        "continuous_df = df[continuous_features]\n",
        "discrete_df = df[discrete_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LKLtaOoMy8N"
      },
      "outputs": [],
      "source": [
        "continuous_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYTr05UNLddq"
      },
      "outputs": [],
      "source": [
        "# As timedelta is only the time difference between the data collection and news publish. So it is not going to add value to the prediction\n",
        "continuous_df = pd.DataFrame(continuous_df)\n",
        "continuous_df = continuous_df.drop(' timedelta', axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogtw3q_7PbRd"
      },
      "outputs": [],
      "source": [
        "# Need to drop the target variable\n",
        "continuous_df = continuous_df.drop(' shares', axis =1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNT8U4-jPuIG"
      },
      "outputs": [],
      "source": [
        "# Need to check the features with negative values. (Skweness)\n",
        "\n",
        "negcols= continuous_df.columns[(continuous_df<=0).any()]\n",
        "negcols"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ilVHtG4uTDtE"
      },
      "source": [
        "As we got almost all of the numerical feature have negative value we have to covert into positive value to apply Box-Cox method to transform the features. (Applied other methods as well but not good)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWxaETkeTDMN"
      },
      "outputs": [],
      "source": [
        "for i in negcols:\n",
        "    m=continuous_df[i].min()\n",
        "    name=i +'_new'\n",
        "    continuous_df[name]=((continuous_df[i]+1)-m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eIuZ9AnUM0V"
      },
      "outputs": [],
      "source": [
        "continuous_df.columns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DXpc3BOlUS_V"
      },
      "source": [
        "We got the new positive columns. Now dropping negative cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNH9WuafUbRV"
      },
      "outputs": [],
      "source": [
        "# Droping old negative column\n",
        "\n",
        "for i in negcols:\n",
        "    continuous_df.drop(i,axis=1,inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iAYGgQnUheW"
      },
      "outputs": [],
      "source": [
        "# Checking negative columns\n",
        "\n",
        "negcols=continuous_df.columns[(continuous_df<=0).any()]\n",
        "negcols"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g1BzvmqQUmFt"
      },
      "source": [
        "Finally we don't have any negative column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAL3bslQUlYM"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "pt=preprocessing.PowerTransformer(method='box-cox',standardize=False)\n",
        "df_num_add=pt.fit_transform(continuous_df)\n",
        "df_num_add=(pd.DataFrame(continuous_df,columns=continuous_df.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzaAQHaBU-YC"
      },
      "outputs": [],
      "source": [
        "for col in df_num_add.columns:\n",
        "    percentiles = df_num_add[col].quantile([0.01,0.99]).values\n",
        "    df_num_add[col][df_num_add[col] <= percentiles[0]] = percentiles[0]\n",
        "    df_num_add[col][df_num_add[col] >= percentiles[1]] = percentiles[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVtSxrO1VK4V"
      },
      "outputs": [],
      "source": [
        "# Checking outliers again\n",
        "\n",
        "num_cols = df_num_add.select_dtypes(['int64','float64']).columns\n",
        "\n",
        "for column in num_cols:    \n",
        "    q1 = df_num_add[column].quantile(0.25)    # First Quartile\n",
        "    q3 = df_num_add[column].quantile(0.75)    # Third Quartile\n",
        "    IQR = q3 - q1                            # Inter Quartile Range\n",
        "\n",
        "    llimit = q1 - 1.5*IQR                       # Lower Limit\n",
        "    ulimit = q3 + 1.5*IQR                        # Upper Limit\n",
        "\n",
        "    outliers = df_num_add[(df_num_add[column] < llimit) | (df_num_add[column] > ulimit)]\n",
        "    print('Number of outliers in \"' + column + '\" : ' + str(len(outliers)))\n",
        "    print(llimit)\n",
        "    print(ulimit)\n",
        "    print(IQR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9vmmaBgfvMv"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "\n",
        "# Define a custom color palette\n",
        "palette = sns.color_palette(\"husl\")\n",
        "\n",
        "# Plotting boxplots for each continuous feature\n",
        "for feature in df_num_add:\n",
        "    plt.figure(figsize=(8, 5))  # This will create a new figure for each feature with custom size\n",
        "    sns.boxplot(data=df_num_add, x=feature, color=palette[3], width=0.5)\n",
        "\n",
        "    # Customize the plot appearance\n",
        "    plt.title(f'Boxplot of {feature}', fontsize=16)\n",
        "    plt.xlabel(feature, fontsize=14)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i9s2nDngMxa"
      },
      "outputs": [],
      "source": [
        "df_num_add.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fw0RlLxgT-Z"
      },
      "outputs": [],
      "source": [
        "discrete_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovuV1HzFgcV3"
      },
      "outputs": [],
      "source": [
        "my_final_df = pd.concat([df_num_add,discrete_df], axis =1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHfuuUJ5jPLG"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4ikqAdBjpVW"
      },
      "outputs": [],
      "source": [
        "df[' shares'].describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7sllas8KjxWQ"
      },
      "source": [
        "As we need to set the threshold that is the news is popular enough. To know that we are taking median of number of total shares on particular news article i.e 1400. If the shares go beyond the 1400 shares that means it is popular otherwise not. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ4s60NTicIz"
      },
      "outputs": [],
      "source": [
        "my_final_df['popularity'] = df[' shares'].apply(lambda x: 0 if x <1400 else 1)\n",
        "my_final_df['shares'] = df[' shares']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fGXbpcZjgLX"
      },
      "outputs": [],
      "source": [
        "my_final_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KRGAaUWYvw4M"
      },
      "source": [
        "As we need to normalize the data before performing t-test to check statistically that their is huge difference between top 5% and bottom 95% mean.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUdtAm42DV4p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Calculate the 95th percentile value for shares\n",
        "shares_95th_percentile = np.percentile(my_final_df['shares'], 95)\n",
        "\n",
        "# Create two groups: top 5% shares and the rest\n",
        "top_5_percent_shares = my_final_df[my_final_df['shares'] > shares_95th_percentile]\n",
        "remaining_shares = my_final_df[my_final_df['shares'] <= shares_95th_percentile]\n",
        "\n",
        "# Perform t-test for a specific feature (e.g., n_tokens_title)\n",
        "t_stat, p_value = ttest_ind(top_5_percent_shares[' n_tokens_title'], remaining_shares[' n_tokens_title'])\n",
        "\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg_h9Q3myeBH"
      },
      "outputs": [],
      "source": [
        "# t-test\n",
        "t_stat, p_value = ttest_ind(top_5_percent_shares[' num_imgs_new'], remaining_shares[' num_imgs_new'])\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwLOTyXCznu4"
      },
      "outputs": [],
      "source": [
        "my_final_df[' num_imgs_new'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8arVu_Qmz2qA"
      },
      "outputs": [],
      "source": [
        "total_outliers_img = my_final_df[my_final_df[' num_imgs_new'] > 5]\n",
        "print(\"total outliers in num_imgs_new: \", len(total_outliers_img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao4UM2us2K0y"
      },
      "outputs": [],
      "source": [
        "# t-test\n",
        "t_stat, p_value = ttest_ind(top_5_percent_shares[' kw_min_min_new'], remaining_shares[' kw_min_min_new'])\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnZ9fzor2foZ"
      },
      "outputs": [],
      "source": [
        "# t-test\n",
        "t_stat, p_value = ttest_ind(top_5_percent_shares[' kw_max_min_new'], remaining_shares[' kw_max_min_new'])\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3cVuyvc2vag"
      },
      "outputs": [],
      "source": [
        "# t-test\n",
        "t_stat, p_value = ttest_ind(top_5_percent_shares[' title_sentiment_polarity_new'], remaining_shares[' title_sentiment_polarity_new'])\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QNVaHtxa2upA"
      },
      "source": [
        "Yes we can say that we have outliers in some of the features. So to reduce the effect of it we will transform our data to minimize the effect of outliers. And we will do PCA also to reduce dimentionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUWAG9Ld4q8J"
      },
      "outputs": [],
      "source": [
        "# Renaming the features to get better understanding\n",
        "my_final_df.columns = my_final_df.columns.str.replace(' ', '')\n",
        "my_final_df.rename(columns = {\" n_tokens_title\": \"n_tokens_title\", \" average_token_length\" :\t\"average_token_length\", \n",
        "                              \" num_keywords\" :\"num_keywords\",\t\"n_tokens_content_new\": \"n_tokens_content\",\t\"n_unique_tokens_new\" : \"n_unique_tokens\",\n",
        "                              \"n_non_stop_words_new\": \"n_non_stop_words\" ,\t\"n_non_stop_unique_tokens_new\" : \"n_non_stop_unique_tokens\",\t\"num_hrefs_new\" : \"num_hrefs\",\n",
        "                              \"num_self_hrefs_new\" : \"num_self_hrefs\"\t,\"num_imgs_new\": \"num_imgs\",\t\"num_videos_new\" :\"num_videos\",\t\"kw_min_min_new\": \"kw_min_min\",\t\n",
        "                              \"kw_max_min_new\" :\t\"kw_max_min\", \"kw_avg_min_new\": \"kw_avg_min\",\t\"self_reference_min_shares_new\" : \"self_reference_min_shares\",\t\"self_reference_max_shares_new\": \"self_reference_max_shares\",\n",
        "                              \"self_reference_avg_sharess_new\": \"self_reference_avg_sharess\",\t\"LDA_00_new\":\"LDA_00\", \t\"LDA_01_new\": \"LDA_01\",\t\"LDA_02_new\": \"LDA_02\",\t\n",
        "                              \"LDA_03_new\": \"LDA_03\",\t\"LDA_04_new\": \"LDA_04\",\t\"global_subjectivity_new\": \"global_subjectivity\",\t\"global_sentiment_polarity_new\": \"global_sentiment_polarity\",\n",
        "                              \"global_rate_positive_words_new\": \"global_rate_positive_words\",\t\"global_rate_negative_words_new\" : \"global_rate_negative_words\", \t\n",
        "                              \"rate_positive_words_new\": \"rate_positive_words\",\t\"rate_negative_words_new\" :\"rate_negative_words\",\t\"avg_positive_polarity_new\":\"avg_positive_polarity\",\n",
        "                              \"min_positive_polarity_new\": \"min_positive_polarity\", \"max_positive_polarity_new\": \"max_positive_polarity\",\t\"avg_negative_polarity_new\": \"avg_negative_polarity\",\n",
        "                              \"min_negative_polarity_new\": \"min_negative_polarity\",\t\"max_negative_polarity_new\" : \"max_negative_polarity\",\t\"title_subjectivity_new\" : \"title_subjectivity\",\n",
        "                              \"title_sentiment_polarity_new\": \"title_sentiment_polarity\", \"abs_title_subjectivity_new\" : \"abs_title_subjectivity\",\t\"abs_title_sentiment_polarity_new\" : \"abs_title_sentiment_polarity\",\n",
        "                              'kw_min_min' : 'worstkw_min', 'kw_max_min' : 'worstkw_max', 'kw_avg_min' : 'worstkw_avg',                                          \n",
        "                          'kw_min_max_new' : 'bestkw_min', 'kw_max_max_new' : 'bestkw_max', 'kw_avg_max_new' : 'bestkw_avg', \n",
        "                          'kw_min_avg_new' : 'avgkw_min', 'kw_max_avg_new' : 'avgkw_max', 'kw_avg_avg_new' : 'avgkw_avg'}, inplace = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9j6ZJ9iCc8-H"
      },
      "source": [
        "Finally we have changed the variables names. Now data is prepared for further analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrUy_1j5dpU4"
      },
      "outputs": [],
      "source": [
        "my_final_df.to_csv('Online_news_popularity_final_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvzyfy6LSZHE"
      },
      "outputs": [],
      "source": [
        "files.download('Online_news_popularity_final_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyhH-phfOHe0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
